{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#folder = \"C:/cppd/D/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\"\n",
    "folder = \"D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\"\n",
    "#folder = \"D:/Project/New_Project/Dataset/New_Dataset in folders\"\n",
    "\n",
    "path_log = \"D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/log.txt\"\n",
    "\n",
    "featuresFileName = \"featuresPSI.tsv\"\n",
    "statisticNumbersFileName = \"statisticsPSI.txt\"\n",
    "\n",
    "snipFolder = \"\"\n",
    "\n",
    "tokens = []\n",
    "\n",
    "token_matrix = []\n",
    "\n",
    "files_list = []\n",
    "token_list = []\n",
    "value_list = []\n",
    "\n",
    "value_matrix = []\n",
    "value_files_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Counter(object):\n",
    "    def __init__(self, lines, files, snippets):\n",
    "        self.lines = lines\n",
    "        self.files = files\n",
    "        self.snippets = snippets\n",
    "\n",
    "    def incLine(self):\n",
    "        self.lines = self.lines + 1\n",
    "\n",
    "    def incFile(self):\n",
    "        self.files = self.files + 1\n",
    "\n",
    "    def incSnippets(self):\n",
    "        self.snippets = self.snippets + 1\n",
    "\n",
    "    def getLines(self):\n",
    "        return self.lines\n",
    "\n",
    "    def getFiles(self):\n",
    "        return self.files\n",
    "\n",
    "    def getSnippets(self):\n",
    "        return self.snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extractor_token():\n",
    "    counter = Counter(0, 0, 0)\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder, file)\n",
    "        #path = folder + \"/\" + file\n",
    "        #path.replace(' ', '_')\n",
    "        if os.path.isfile(path) and path.endswith(\".txt\"):\n",
    "            counter.incFile()\n",
    "            extractTokenList(path, file, snipFolder, counter)\n",
    "        elif os.path.isdir(path):\n",
    "            createSnippetsDir(path, file, snipFolder, counter)\n",
    "    #print(token_matrix)\n",
    "    #print(tokens)\n",
    "    #print(len(tokens))\n",
    "    #print(counter.getFiles())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSnippetsDir(folder, repo, snipFolder, counter):\n",
    "    try:\n",
    "        for file in os.listdir(folder):\n",
    "            path = os.path.join(folder, file)\n",
    "            #path = folder + \"/\" + file\n",
    "            #path.replace(' ', '_')\n",
    "            if os.path.isfile(path) and path.endswith(\".txt\"):\n",
    "                counter.incFile()\n",
    "                extractTokenList(path, file, snipFolder, counter)\n",
    "            elif os.path.isdir(path):\n",
    "                createSnippetsDir(path, repo, snipFolder, counter)\n",
    "    except:\n",
    "        with open('experiment/bad_files.txt', 'w') as f:\n",
    "            f.write(folder + \"/\" + file)\n",
    "            f.write(\"\\n\")\n",
    "        pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "\n",
    "def extractTokenList(path, file, snipFolder, counter):\n",
    "    #f = open(path, 'r')\n",
    "    f = codecs.open(path, \"r\", \"utf_8_sig\" )\n",
    "    #fileOutput = open(path_log, 'w')\n",
    "    #fileOutput.write(path)\n",
    "    #fileOutput.write(\"\\n\")\n",
    "    #fileOutput.close()\n",
    "    token_types = []\n",
    "    value_types = []\n",
    "    for line in f:\n",
    "        token_types.append(line.split(' (')[0])\n",
    "        value_types.append(line.split(' (', 1)[1].rsplit(')', 1)[0])\n",
    "    #token_types = [line.split(' (')[0] for line in f]\n",
    "    #value_types = [(line.split(' (\\'', 1)[1]).rsplit('\\')', 1)[0] for line in f]\n",
    "    #token_types = [split_unicode_chrs(line)[0] for line in f]\n",
    "    #token_matrix.append(token_types)\n",
    "    files_list.append(path)\n",
    "    #value_matrix.append(value_types)\n",
    "    #value_files_list.append(path)\n",
    "    #print(token_types)\n",
    "    with open(path + \".token\", 'w') as tsv_file_token:\n",
    "        tsv_writer = csv.writer(tsv_file_token, delimiter=' ', lineterminator='\\n')\n",
    "        tsv_writer.writerow(token_types)\n",
    "    \n",
    "    with open(path + \".value\", 'w') as tsv_file_value:\n",
    "        tsv_writer = csv.writer(tsv_file_value, delimiter=' ', lineterminator='\\n')\n",
    "        tsv_writer.writerow(value_types)\n",
    "    \n",
    "\n",
    "    #print(token_types)\n",
    "    #for t in token_types:\n",
    "    #    if t not in tokens:\n",
    "    #        tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_extractor_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources\\\\example.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\androidTest\\\\java\\\\com\\\\ground0\\\\transaction\\\\ExampleInstrumentedTest.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\components\\\\BaseActivity.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\components\\\\BaseFragment.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\components\\\\RetailApplication.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\livedata\\\\SingleLiveEvent.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\db\\\\dao\\\\RetailTransactionDao.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\db\\\\LocalStore.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\db\\\\RoomDatabase.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\db\\\\util\\\\DateTypeConverter.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\db\\\\util\\\\LocalDateTimeConverter.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\network\\\\ApiStore.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\network\\\\CloudStore.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\network\\\\util\\\\HttpStatusOperator.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\network\\\\util\\\\ResponseStatusOperator.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\Repository.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\RepositoryImp.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\Main2Activity.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\MainActivity.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\transaction\\\\TransactionListActivity.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\transaction\\\\TransactionListAdapter.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\transaction\\\\TransactionListItemViewModelFactory.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\transaction\\\\transaction\\\\TransactionListViewModel.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\test\\\\java\\\\com\\\\ground0\\\\transaction\\\\core\\\\repository\\\\network\\\\util\\\\HttpStatusOperatorTest.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\app\\\\src\\\\test\\\\java\\\\com\\\\ground0\\\\transaction\\\\ExampleUnitTest.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\model\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\model\\\\Customer.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\model\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\model\\\\Retailer.txt.token', 'D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\\\sources_2\\\\sources\\\\model\\\\src\\\\main\\\\java\\\\com\\\\ground0\\\\model\\\\RetailTransaction.txt.token']\n"
     ]
    }
   ],
   "source": [
    "token_list = list(map(lambda x: x + \".token\", files_list))\n",
    "value_list = list(map(lambda x: x + \".value\", files_list))\n",
    "\n",
    "import pickle\n",
    "\n",
    "class DataFilePath:\n",
    "    def __setitem__(self, index: int, value):\n",
    "        '''сохранить в файл index.data'''\n",
    "        with open('experiment/{}.data'.format(index), 'wb') as f:\n",
    "            pickle.dump(value, f)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''читать из файла index.data'''\n",
    "        with open('experiment/{}.data'.format(index), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "file = DataFilePath()\n",
    "\n",
    "file[10] = token_list\n",
    "file[11] = value_list\n",
    "\n",
    "d1 = file[10]\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class DataFilePath:\n",
    "    def __setitem__(self, index: int, value):\n",
    "        '''сохранить в файл index.data'''\n",
    "        with open('experiment/{}.data'.format(index), 'wb') as f:\n",
    "            pickle.dump(value, f)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''читать из файла index.data'''\n",
    "        with open('experiment/{}.data'.format(index), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "file = DataFilePath()\n",
    "\n",
    "d_tokens = file[10]\n",
    "d_values = file[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizerToken = CountVectorizer(input='filename')\n",
    "vectors = vectorizerToken.fit_transform(d_tokens)\n",
    "\n",
    "print(len(vectors.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...,  0  0  4]\n",
      " [ 0  2  0 ...,  1  0 30]\n",
      " [ 0  0  0 ...,  0  1 42]\n",
      " ..., \n",
      " [ 0  6  0 ...,  0  6 46]\n",
      " [ 0  6  0 ...,  0  6 43]\n",
      " [ 0 14  0 ...,  0  7 76]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_1 = vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "#freq   = CountVectorizer()\n",
    "#corpus = freq.fit_transform(corpus)\n",
    "\n",
    "onehot = Binarizer()\n",
    "corpus = onehot.fit_transform(vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "[[0 0 0 ..., 0 0 1]\n",
      " [0 1 0 ..., 1 0 1]\n",
      " [0 0 0 ..., 0 1 1]\n",
      " ..., \n",
      " [0 1 0 ..., 0 1 1]\n",
      " [0 1 0 ..., 0 1 1]\n",
      " [0 1 0 ..., 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_2 = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf  = TfidfVectorizer(input='filename')\n",
    "vec_tf_idf = tfidf.fit_transform(d_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(vec_tf_idf.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.          0.63772348]\n",
      " [ 0.          0.06234209  0.         ...,  0.04076756  0.          0.5635883 ]\n",
      " [ 0.          0.          0.         ...,  0.          0.02970816\n",
      "   0.60431845]\n",
      " ..., \n",
      " [ 0.          0.15279395  0.         ...,  0.          0.19013177\n",
      "   0.70599572]\n",
      " [ 0.          0.15958755  0.         ...,  0.          0.1985855\n",
      "   0.68929564]\n",
      " [ 0.          0.19591551  0.         ...,  0.          0.12189541\n",
      "   0.64097885]]\n"
     ]
    }
   ],
   "source": [
    "print(vec_tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_3 = vec_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arrow', 'at', 'block_comment', 'class', 'closing_quote', 'colon', 'coloncolon', 'comma', 'dot', 'else', 'eol_comment', 'eq', 'eqeq', 'excl', 'false', 'fun', 'gt', 'identifier', 'if', 'integer_literal', 'interface', 'kdoc', 'lbrace', 'lbracket', 'long_template_entry_end', 'long_template_entry_start', 'lpar', 'lt', 'mul', 'null', 'object', 'open_quote', 'package', 'plus', 'quest', 'rbrace', 'rbracket', 'regular_string_part', 'return', 'rpar', 'super', 'this', 'true', 'val', 'var', 'white_space']\n",
      "['arrow', 'at', 'block_comment', 'class', 'closing_quote', 'colon', 'coloncolon', 'comma', 'dot', 'else', 'eol_comment', 'eq', 'eqeq', 'excl', 'false', 'fun', 'gt', 'identifier', 'if', 'integer_literal', 'interface', 'kdoc', 'lbrace', 'lbracket', 'long_template_entry_end', 'long_template_entry_start', 'lpar', 'lt', 'mul', 'null', 'object', 'open_quote', 'package', 'plus', 'quest', 'rbrace', 'rbracket', 'regular_string_part', 'return', 'rpar', 'super', 'this', 'true', 'val', 'var', 'white_space']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizerToken.get_feature_names())\n",
    "#print(onehot.get_feature_names())\n",
    "print(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizerToken.get_feature_names()\n",
    "res_1_names = [\"count_\" + feature for feature in feature_names]\n",
    "res_2_names = [\"bin_\" + feature for feature in feature_names]\n",
    "res_3_names = [\"tfidf_\" + feature for feature in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['count_arrow', 'count_at', 'count_block_comment', 'count_class', 'count_closing_quote', 'count_colon', 'count_coloncolon', 'count_comma', 'count_dot', 'count_else', 'count_eol_comment', 'count_eq', 'count_eqeq', 'count_excl', 'count_false', 'count_fun', 'count_gt', 'count_identifier', 'count_if', 'count_integer_literal', 'count_interface', 'count_kdoc', 'count_lbrace', 'count_lbracket', 'count_long_template_entry_end', 'count_long_template_entry_start', 'count_lpar', 'count_lt', 'count_mul', 'count_null', 'count_object', 'count_open_quote', 'count_package', 'count_plus', 'count_quest', 'count_rbrace', 'count_rbracket', 'count_regular_string_part', 'count_return', 'count_rpar', 'count_super', 'count_this', 'count_true', 'count_val', 'count_var', 'count_white_space']\n"
     ]
    }
   ],
   "source": [
    "print(res_1_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "#print(token_list)\n",
    "print(len(token_list))\n",
    "print(len(res_1_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  1  0  9  0  6 39  0  0  1  0  2  0  2  2 91  0  0  0  1  4  0  0\n",
      "  0  5  2  0  0  1  0  1  0  8  4  0  0  2  5  0  0  0  1  0 63]\n",
      "[0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
      " 0 1 1 0 0 0 1 0 1]\n",
      "[ 0.          0.          0.          0.00986373  0.          0.08572636\n",
      "  0.          0.0793603   0.3348223   0.          0.          0.01137593\n",
      "  0.          0.06094777  0.          0.01972747  0.02449962  0.75476629\n",
      "  0.          0.          0.          0.00952515  0.0355473   0.          0.\n",
      "  0.          0.04292594  0.02449962  0.          0.          0.0200833   0.\n",
      "  0.00858519  0.          0.13067579  0.0355473   0.          0.\n",
      "  0.04272369  0.04292594  0.          0.          0.          0.01799886\n",
      "  0.          0.52253051]\n",
      "28\n",
      "28\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(res_1[10])\n",
    "print(res_2[10])\n",
    "print(res_3[10])\n",
    "print(len(res_1))\n",
    "print(len(d_tokens))\n",
    "print(type(res_1[0]))\n",
    "print(type([d_tokens[i]]))\n",
    "print(type([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresFileName = \"experiment/features_token.tsv\"\n",
    "k = len(d_tokens)\n",
    "\n",
    "with open(featuresFileName, 'w') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t', lineterminator='\\n')\n",
    "    tsv_writer.writerow([\"ID\", \"Path\"] + res_1_names + res_2_names + res_3_names)\n",
    "    for i in range(0, k):\n",
    "        tsv_writer.writerow([i + 1] + [d_tokens[i]] + list(res_1[i]) + list(res_2[i]) + list(res_3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizerToken = CountVectorizer(input='filename')\n",
    "vectors = vectorizerToken.fit_transform(d_values)\n",
    "\n",
    "print(len(vectors.toarray()))\n",
    "\n",
    "res_1 = vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "#freq   = CountVectorizer()\n",
    "#corpus = freq.fit_transform(corpus)\n",
    "\n",
    "onehot = Binarizer()\n",
    "corpus = onehot.fit_transform(vectors.toarray())\n",
    "\n",
    "res_2 = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['count_00', 'count_04', 'count_05', 'count_06', 'count_07', 'count_08', 'count_10', 'count_18', 'count_2017', 'count_404', 'count_a_main2_text', 'count_a_main_button', 'count_a_main_container', 'count_a_main_text', 'count_a_transaction_list_container', 'count_a_transaction_list_recycler', 'count_abstract', 'count_active', 'count_activity_main', 'count_activity_main2', 'count_activity_transaction_list', 'count_adapt', 'count_adapter', 'count_addcalladapterfactory', 'count_addconverterfactory', 'count_addition_iscorrect', 'count_after', 'count_agreed', 'count_also', 'count_amount', 'count_amount_cents', 'count_amount_currency', 'count_an', 'count_and', 'count_android', 'count_androidjunit4', 'count_androidschedulers', 'count_androidthreeten', 'count_annotation', 'count_annotations', 'count_any', 'count_apache', 'count_api_version', 'count_apistore', 'count_app', 'count_appcompatactivity', 'count_appcontext', 'count_applicable', 'count_application', 'count_apply', 'count_arch', 'count_as', 'count_assert', 'count_assertequals', 'count_asstring', 'count_at', 'count_atomic', 'count_atomicboolean', 'count_avoids', 'count_aware', 'count_baseactivity', 'count_basefragment', 'count_baseurl', 'count_basis', 'count_be', 'count_bind', 'count_bindview', 'count_body', 'count_bp', 'count_build', 'count_buildconfig', 'count_builder', 'count_bundle', 'count_but', 'count_butterknife', 'count_by', 'count_call', 'count_called', 'count_calledq', 'count_calls', 'count_can', 'count_canonicalname', 'count_cases', 'count_change', 'count_changes', 'count_class', 'count_clazz', 'count_cleaner', 'count_cloudstore', 'count_columninfo', 'count_com', 'count_common', 'count_companion', 'count_compareandset', 'count_compliance', 'count_components', 'count_concurrent', 'count_conditions', 'count_configuration', 'count_const', 'count_constructor', 'count_content', 'count_context', 'count_converter', 'count_copy', 'count_copyright', 'count_core', 'count_create', 'count_created', 'count_created_at', 'count_createdat', 'count_createitemviewmodel', 'count_currency', 'count_currenttimemillis', 'count_customer', 'count_customer_id', 'count_customerid', 'count_customers', 'count_dao', 'count_database', 'count_databasebuilder', 'count_databaseimp', 'count_databinding', 'count_date', 'count_datetimeformatter', 'count_datetypeconverter', 'count_db', 'count_deserialize', 'count_design', 'count_development', 'count_device', 'count_disposable', 'count_disposables', 'count_distributed', 'count_documentation', 'count_done', 'count_dum', 'count_either', 'count_else', 'count_email', 'count_emitted', 'count_entities', 'count_entity', 'count_error', 'count_errorevent', 'count_events', 'count_example', 'count_exampleinstrumentedtest', 'count_exampleunittest', 'count_except', 'count_execute', 'count_explicit', 'count_express', 'count_fail', 'count_false', 'count_file', 'count_findviewbyid', 'count_flowable', 'count_for', 'count_format', 'count_formatter', 'count_fragment', 'count_from', 'count_fromcallable', 'count_fun', 'count_function', 'count_functions', 'count_get', 'count_getcustomer', 'count_getcustomers', 'count_getitemcount', 'count_getnon', 'count_getretailer', 'count_getretailers', 'count_gettargetcontext', 'count_gettransaction', 'count_gettransactions', 'count_going', 'count_google', 'count_governing', 'count_ground0', 'count_gson', 'count_gsonbuilder', 'count_gsonconverterfactory', 'count_hasactiveobservers', 'count_hello', 'count_holder', 'count_host', 'count_http', 'count_httpstatusoperator', 'count_httpstatusoperatortest', 'count_id', 'count_if', 'count_implemented', 'count_implied', 'count_import', 'count_in', 'count_inc', 'count_inflate', 'count_init', 'count_initrecyclerview', 'count_initviewmodel', 'count_inner', 'count_insert', 'count_instrumentationregistry', 'count_instrumented', 'count_int', 'count_intent', 'count_interface', 'count_internal', 'count_io', 'count_is', 'count_iso_offset_date_time', 'count_issuccessful', 'count_it', 'count_item_transaction', 'count_itemtransactionbinding', 'count_itemview', 'count_itemviewmodel', 'count_jakewharton', 'count_java', 'count_javaclass', 'count_jointostring', 'count_json', 'count_jsondeserializationcontext', 'count_jsondeserializer', 'count_jsonelement', 'count_jsonprimitive', 'count_jsonserializationcontext', 'count_jsonserializer', 'count_junit', 'count_kind', 'count_kotlin', 'count_kotlinx', 'count_lang', 'count_language', 'count_lateinit', 'count_law', 'count_layout', 'count_layoutinflater', 'count_layoutmanager', 'count_lazy', 'count_length_short', 'count_let', 'count_license', 'count_licensed', 'count_licenses', 'count_lifecycle', 'count_lifecycleowner', 'count_like', 'count_limit', 'count_limitations', 'count_linearlayoutmanager', 'count_list', 'count_listof', 'count_livedata', 'count_loading', 'count_loadtransactions', 'count_local', 'count_localdatetime', 'count_localdatetimeconverter', 'count_localstore', 'count_log', 'count_long', 'count_machine', 'count_main', 'count_main2activity', 'count_mainactivity', 'count_mainthread', 'count_make', 'count_maketext', 'count_map', 'count_may', 'count_mediatype', 'count_message', 'count_messages', 'count_model', 'count_mpending', 'count_multiple', 'count_mutablelivedata', 'count_name', 'count_navigation', 'count_network', 'count_new', 'count_non_existant', 'count_not', 'count_note', 'count_notified', 'count_notifydatasetchanged', 'count_null', 'count_object', 'count_observable', 'count_observableoperator', 'count_observablesource', 'count_observe', 'count_observeon', 'count_observer', 'count_observers', 'count_obtain', 'count_of', 'count_oh', 'count_okhttp3', 'count_on', 'count_onbindviewholder', 'count_onbuttonclick', 'count_onchanged', 'count_onclick', 'count_onconflict', 'count_onconflictstrategy', 'count_oncreate', 'count_oncreateviewholder', 'count_one', 'count_onerror', 'count_only', 'count_onsubscribe', 'count_onsuccess', 'count_open', 'count_or', 'count_org', 'count_os', 'count_override', 'count_owner', 'count_package', 'count_packagename', 'count_parent', 'count_parse', 'count_permissions', 'count_persistence', 'count_phone', 'count_position', 'count_postcustomers', 'count_postretailer', 'count_postretailers', 'count_posttransaction', 'count_posttransactions', 'count_primarykey', 'count_println', 'count_private', 'count_problem', 'count_protected', 'count_query', 'count_reactivestreams', 'count_reactivex', 'count_recyclerview', 'count_reflect', 'count_registered', 'count_registertypeadapter', 'count_replace', 'count_repository', 'count_repositoryimp', 'count_required', 'count_response', 'count_responsebody', 'count_responsestatusoperator', 'count_restimp', 'count_retail_transactions', 'count_retailapplication', 'count_retailer', 'count_retailer_id', 'count_retailerid', 'count_retailers', 'count_retailtransaction', 'count_retailtransactiondao', 'count_retailtransactions', 'count_retrofit', 'count_retrofit2', 'count_return', 'count_room', 'count_roomdatabase', 'count_rotation', 'count_runner', 'count_runwith', 'count_rx', 'count_rxjava2', 'count_rxjava2calladapterfactory', 'count_savedinstancestate', 'count_schedulers', 'count_see', 'count_select', 'count_sends', 'count_serialize', 'count_serializedname', 'count_set', 'count_setcontentview', 'count_setsupportactionbar', 'count_settings', 'count_setvalue', 'count_shits', 'count_shouldcallonerrorfor404', 'count_shouldcallonsuccessfor200', 'count_show', 'count_showtransactions', 'count_singleliveevent', 'count_singleobserver', 'count_singlesource', 'count_size', 'count_snackbar', 'count_software', 'count_something', 'count_specific', 'count_src', 'count_startactivity', 'count_statusoperator', 'count_string', 'count_subscribe', 'count_subscribeon', 'count_subscriber', 'count_subscribetoerror', 'count_subscribetomessages', 'count_subscribetotransactions', 'count_subscription', 'count_success', 'count_super', 'count_support', 'count_switchmapsingle', 'count_synthetic', 'count_system', 'count_tablename', 'count_tag', 'count_templates', 'count_test', 'count_testing', 'count_text', 'count_textview', 'count_that', 'count_the', 'count_there', 'count_this', 'count_threeten', 'count_threetenabp', 'count_throwable', 'count_to', 'count_toast', 'count_todate', 'count_todo', 'count_toobservable', 'count_toolbar', 'count_tools', 'count_tostring', 'count_transaction', 'count_transaction_db', 'count_transactiondao', 'count_transactionid', 'count_transactionlistactivity', 'count_transactionlistadapter', 'count_transactionlistitemviewmodel', 'count_transactionlistitemviewmodelfactory', 'count_transactionlistviewmodel', 'count_transactions', 'count_triggered', 'count_true', 'count_type', 'count_typeconverter', 'count_typeconverters', 'count_typeofsrc', 'count_typeoft', 'count_under', 'count_unit', 'count_unless', 'count_update', 'count_updated_at', 'count_updatedat', 'count_updates', 'count_use', 'count_useappcontext', 'count_used', 'count_util', 'count_v4', 'count_v7', 'count_val', 'count_value', 'count_var', 'count_version', 'count_vertical', 'count_view', 'count_viewgroup', 'count_viewholder', 'count_viewmodel', 'count_viewmodelproviders', 'count_viewtype', 'count_void', 'count_warranties', 'count_was', 'count_went', 'count_where', 'count_which', 'count_widget', 'count_will', 'count_with', 'count_without', 'count_write', 'count_writing', 'count_wrong', 'count_www', 'count_yo', 'count_you']\n",
      "496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf  = TfidfVectorizer(input='filename')\n",
    "vec_tf_idf = tfidf.fit_transform(d_values)\n",
    "\n",
    "res_3 = vec_tf_idf.toarray()\n",
    "\n",
    "feature_names = vectorizerToken.get_feature_names()\n",
    "res_1_names = [\"count_\" + feature for feature in feature_names]\n",
    "res_2_names = [\"bin_\" + feature for feature in feature_names]\n",
    "res_3_names = [\"tfidf_\" + feature for feature in feature_names]\n",
    "\n",
    "print(res_1_names)\n",
    "\n",
    "print(len(res_1_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresFileName = \"experiment/features_value.tsv\"\n",
    "k = len(d_values)\n",
    "\n",
    "with open(featuresFileName, 'w') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t', lineterminator='\\n')\n",
    "    tsv_writer.writerow([\"ID\", \"Path\"] + res_1_names + res_2_names + res_3_names)\n",
    "    for i in range(0, k):\n",
    "        tsv_writer.writerow([i + 1] + [d_values[i]] + list(res_1[i]) + list(res_2[i]) + list(res_3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "16\n",
      "250\n",
      "284\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "#print(tokens)\n",
    "#print(token_matrix)\n",
    "#print(files_list)\n",
    "\n",
    "print(len(token_matrix))\n",
    "print(len(token_matrix[0]))\n",
    "print(len(token_matrix[10]))\n",
    "print(len(token_matrix[20]))\n",
    "print(len(files_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['package', 'WHITE_SPACE', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'WHITE_SPACE', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'DOT', 'IDENTIFIER', 'WHITE_SPACE', 'KDoc', 'WHITE_SPACE', 'interface', 'WHITE_SPACE', 'IDENTIFIER', 'WHITE_SPACE', 'LBRACE', 'WHITE_SPACE', 'AT', 'IDENTIFIER', 'LPAR', 'OPEN_QUOTE', 'REGULAR_STRING_PART', 'CLOSING_QUOTE', 'RPAR', 'WHITE_SPACE', 'fun', 'WHITE_SPACE', 'IDENTIFIER', 'LPAR', 'RPAR', 'COLON', 'WHITE_SPACE', 'IDENTIFIER', 'LT', 'IDENTIFIER', 'LT', 'IDENTIFIER', 'LT', 'IDENTIFIER', 'GT', 'GT', 'GT', 'WHITE_SPACE', 'AT', 'IDENTIFIER', 'LPAR', 'OPEN_QUOTE', 'REGULAR_STRING_PART', 'CLOSING_QUOTE', 'RPAR', 'WHITE_SPACE', 'fun', 'WHITE_SPACE', 'IDENTIFIER', 'LPAR', 'RPAR', 'COLON', 'WHITE_SPACE', 'IDENTIFIER', 'LT', 'IDENTIFIER', 'LT', 'IDENTIFIER', 'GT', 'GT', 'WHITE_SPACE', 'AT', 'IDENTIFIER', 'LPAR', 'OPEN_QUOTE', 'REGULAR_STRING_PART', 'CLOSING_QUOTE', 'RPAR', 'WHITE_SPACE', 'fun', 'WHITE_SPACE', 'IDENTIFIER', 'LPAR', 'AT', 'IDENTIFIER', 'LPAR', 'OPEN_QUOTE', 'REGULAR_STRING_PART', 'CLOSING_QUOTE', 'RPAR', 'WHITE_SPACE', 'IDENTIFIER', 'COLON', 'WHITE_SPACE', 'IDENTIFIER', 'RPAR', 'COLON', 'WHITE_SPACE', 'IDENTIFIER', 'LT', 'IDENTIFIER', 'LT', 'IDENTIFIER', 'GT', 'GT', 'WHITE_SPACE', 'RBRACE']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class DataFileTokens:\n",
    "    def __setitem__(self, index: int, value):\n",
    "        '''сохранить в файл index.data'''\n",
    "        with open('experiment/tokens/{}.data'.format(index), 'wb') as f:\n",
    "            pickle.dump(value, f)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''читать из файла index.data'''\n",
    "        with open('experiment/tokens/{}.data'.format(index), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "file = DataFileTokens()\n",
    "\n",
    "for i in range(0, len(token_matrix)):\n",
    "    file[i] = token_matrix[i]\n",
    "\n",
    "d1 = file[11]\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Project/CodePerformanceProblemDetection/5_1_FeaturesTokenExtractor/data\\sources_2\\sources\\app\\src\\androidTest\\java\\com\\ground0\\transaction\\ExampleInstrumentedTest.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class DataFilePath:\n",
    "    def __setitem__(self, index: int, value):\n",
    "        '''сохранить в файл index.data'''\n",
    "        with open('experiment/files/{}.data'.format(index), 'wb') as f:\n",
    "            pickle.dump(value, f)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''читать из файла index.data'''\n",
    "        with open('experiment/files/{}.data'.format(index), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "file = DataFilePath()\n",
    "\n",
    "file[0] = files_list\n",
    "\n",
    "d1 = file[0][1]\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 10 [-3]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class DataFile:\n",
    "    def __setitem__(self, index: int, value):\n",
    "        '''сохранить в файл index.data'''\n",
    "        with open('{}.data'.format(index), 'wb') as f:\n",
    "            pickle.dump(value, f)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''читать из файла index.data'''\n",
    "        with open('{}.data'.format(index), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "file = DataFile()\n",
    "# записать в файл file[индекс] - индекс.data\n",
    "file[0] = [[2, 1, 3, 4, 5], [11, 12, 13, 14, 15], [21, 22, 23, 24, 25]]\n",
    "file[1] = 10\n",
    "file[2] = {'123': [-1, -2, [-3]]}\n",
    "# читать из file[индекс] - индекс.data\n",
    "d1 = file[0][1][-1]  # 15\n",
    "d2 = file[1]  # 10\n",
    "d3 = file[2]['123'][2]  # [-3]\n",
    "print(d1, d2, d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value_matrix = []\n",
    "value_files_list = []\n",
    "\n",
    "def feature_extractor_value():\n",
    "    counter = Counter(0, 0, 0)\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder, file)\n",
    "        #path = folder + \"/\" + file\n",
    "        #path.replace(' ', '_')\n",
    "        if os.path.isfile(path) and path.endswith(\".txt\"):\n",
    "            counter.incFile()\n",
    "            extractTokenList(path, file, snipFolder, counter)\n",
    "        elif os.path.isdir(path):\n",
    "            createSnippetsDir(path, file, snipFolder, counter)\n",
    "    #print(token_matrix)\n",
    "    #print(tokens)\n",
    "    #print(len(tokens))\n",
    "    #print(counter.getFiles())\n",
    "    return\n",
    "\n",
    "def createSnippetsDir(folder, repo, snipFolder, counter):\n",
    "    try:\n",
    "        for file in os.listdir(folder):\n",
    "            path = os.path.join(folder, file)\n",
    "            #path = folder + \"/\" + file\n",
    "            #path.replace(' ', '_')\n",
    "            if os.path.isfile(path) and path.endswith(\".txt\"):\n",
    "                counter.incFile()\n",
    "                extractTokenList(path, file, snipFolder, counter)\n",
    "            elif os.path.isdir(path):\n",
    "                createSnippetsDir(path, repo, snipFolder, counter)\n",
    "    except:\n",
    "        print(folder + \"/\" + file)\n",
    "        print()\n",
    "        pass\n",
    "    return\n",
    "\n",
    "import codecs\n",
    "\n",
    "def extractTokenList(path, file, snipFolder, counter):\n",
    "    #f = open(path, 'r')\n",
    "    f = codecs.open(path, \"r\", \"utf_8_sig\" )\n",
    "    #fileOutput = open(path_log, 'w')\n",
    "    #fileOutput.write(path)\n",
    "    #fileOutput.write(\"\\n\")\n",
    "    #fileOutput.close()\n",
    "    value_types = [(line.split(' (\\'', 1)[1]).rsplit('\\')', 1)[0] for line in f]\n",
    "    #token_types = [split_unicode_chrs(line)[0] for line in f]\n",
    "    value_matrix.append(value_types)\n",
    "    value_files_list.append(path)\n",
    "    #print(token_types)\n",
    "    #for t in token_types:\n",
    "    #    if t not in tokens:\n",
    "    #        tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extractor_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['package', ' ', 'com', '.', 'ground0', '.', 'transaction', '.', 'core', '.', 'repository', '.', 'db', '.', 'util', '\\\\n\\\\n', 'import', ' ', 'android', '.', 'arch', '.', 'persistence', '.', 'room', '.', 'TypeConverter', '\\\\n', 'import', ' ', 'org', '.', 'threeten', '.', 'bp', '.', 'LocalDateTime', '\\\\n\\\\n', '/**\\\\n * Created by 00-00-00 on 05/05/18.\\\\n */', '\\\\n\\\\n', 'open', ' ', 'class', ' ', 'DateTypeConverter', ' ', '{', '\\\\n\\\\n  ', '@', 'TypeConverter', '\\\\n  ', 'fun', ' ', 'toDate', '(', 'value', ':', ' ', 'String', '?', ')', ':', ' ', 'LocalDateTime', ' ', '=', ' ', 'LocalDateTime', '.', 'parse', '(', 'value', ')', '\\\\n\\\\n  ', '@', 'TypeConverter', '\\\\n  ', 'fun', ' ', 'toString', '(', 'value', ':', ' ', 'LocalDateTime', '?', ')', ':', ' ', 'String', ' ', '=', ' ', 'value', '.', 'toString', '(', ')', '\\\\n\\\\n', '}']\n"
     ]
    }
   ],
   "source": [
    "print(value_matrix[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['package', ' ', 'com', '.', 'ground0', '.', 'transaction', '.', 'core', '.', 'repository', '.', 'network', '\\\\n\\\\n', 'import', ' ', 'com', '.', 'ground0', '.', 'model', '.', 'RetailTransaction', '\\\\n', 'import', ' ', 'io', '.', 'reactivex', '.', 'Flowable', '\\\\n', 'import', ' ', 'io', '.', 'reactivex', '.', 'Observable', '\\\\n', 'import', ' ', 'retrofit2', '.', 'Response', '\\\\n', 'import', ' ', 'retrofit2', '.', 'http', '.', 'GET', '\\\\n', 'import', ' ', 'retrofit2', '.', 'http', '.', 'Query', '\\\\n\\\\n', '/**\\\\n * Created by 00-00-00 on 05/05/18.\\\\n */', '\\\\n\\\\n', 'interface', ' ', 'ApiStore', ' ', '{', '\\\\n\\\\n  ', '@', 'GET', '(', '\"', 'transactions', '\"', ')', '\\\\n  ', 'fun', ' ', 'getTransactions', '(', ')', ':', ' ', 'Observable', '<', 'Response', '<', 'List', '<', 'RetailTransaction', '>', '>', '>', '\\\\n\\\\n  ', '@', 'GET', '(', '\"', 'non_existant', '\"', ')', '\\\\n  ', 'fun', ' ', 'getNon', '(', ')', ':', ' ', 'Observable', '<', 'Response', '<', 'Unit', '>', '>', '\\\\n\\\\n  ', '@', 'GET', '(', '\"', 'transaction/{transactionId}', '\"', ')', '\\\\n  ', 'fun', ' ', 'getTransaction', '(', '@', 'Query', '(', '\"', 'transactionId', '\"', ')', ' ', 'id', ':', ' ', 'Long', ')', ':', ' ', 'Observable', '<', 'Response', '<', 'RetailTransaction', '>', '>', '\\\\n\\\\n', '}']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class DataFileValues:\n",
    "    def __setitem__(self, index: int, value):\n",
    "        '''сохранить в файл index.data'''\n",
    "        with open('experiment/values/{}.data'.format(index), 'wb') as f:\n",
    "            pickle.dump(value, f)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        '''читать из файла index.data'''\n",
    "        with open('experiment/values/{}.data'.format(index), 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "file = DataFileValues()\n",
    "\n",
    "for i in range(0, len(value_matrix)):\n",
    "    file[i] = value_matrix[i]\n",
    "\n",
    "d1 = file[11]\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "vectors = vectorizer.fit_transform(token_matrix)\n",
    "\n",
    "print(len(vectors.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 2 0 0 1 0 0 1 0 1 2 4 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "46\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "res = vectors.toarray()\n",
    "\n",
    "print(res[0])\n",
    "print(len(res[0]))\n",
    "print(len(res[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARROW', 'AT', 'BLOCK_COMMENT', 'CLOSING_QUOTE', 'COLON', 'COLONCOLON', 'COMMA', 'DOT', 'EOL_COMMENT', 'EQ', 'EQEQ', 'EXCL', 'GT', 'IDENTIFIER', 'INTEGER_LITERAL', 'KDoc', 'LBRACE', 'LBRACKET', 'LONG_TEMPLATE_ENTRY_END', 'LONG_TEMPLATE_ENTRY_START', 'LPAR', 'LT', 'MUL', 'OPEN_QUOTE', 'PLUS', 'QUEST', 'RBRACE', 'RBRACKET', 'REGULAR_STRING_PART', 'RPAR', 'WHITE_SPACE', 'class', 'else', 'false', 'fun', 'if', 'interface', 'null', 'object', 'package', 'return', 'super', 'this', 'true', 'val', 'var']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizerValue = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "vectorsValue = vectorizerValue.fit_transform(value_matrix)\n",
    "\n",
    "print(len(vectorsValue.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "470\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "res = vectorsValue.toarray()\n",
    "\n",
    "print(res[0])\n",
    "print(len(res[0]))\n",
    "print(len(res[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42  2  0  0  5  5  0  0  6  0 39  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  9  0  2  1  0  2  8  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  3  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  2  2  3  2  2  2  0  0  0\n",
      "  0  0  0  0  6  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0\n",
      "  0  0  0  0  0  0  9  3  3  0  0  0  0  0  0  0  0  3  3  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  0  0  0  0  2  0  0  0  0  0  1  0  7  0  1  0  0  0  0  0  0\n",
      "  2  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0\n",
      "  0  0  0  0  0  2  0  0  2  0  0  0  0  0  0  0  0  0  0  0  6  1  6  0  0\n",
      "  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  2  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  2  0  1  0  0  1  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  2  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "  0  1  1  0  0  0  1  0  0  1  0  0  0  0  0  0  0  0  4  4]\n"
     ]
    }
   ],
   "source": [
    "print(res[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '${', '(', ')', '*', '+', ',', '->', '.', '/', '/**\\\\n   * Used for cases where T is Void, to make calls cleaner.\\\\n   */', \"/**\\\\n * A lifecycle-aware observable that sends only new updates after subscription, used for events like\\\\n * navigation and Snackbar messages.\\\\n *\\\\n *\\\\n * This avoids a common problem with events: on configuration change (like rotation) an update\\\\n * can be emitted if the observer is active. This LiveData only calls the observable if there's an\\\\n * explicit call to setValue() or call().\\\\n *\\\\n *\\\\n * Note that only one observer is going to be notified of changes.\\\\n */\", '/**\\\\n * Created by 00-00-00 on 04/05/18.\\\\n */', '/**\\\\n * Created by 00-00-00 on 05/05/18.\\\\n */', '/**\\\\n * Created by 00-00-00 on 06/05/18.\\\\n */', '/**\\\\n * Created by 00-00-00 on 07/05/18.\\\\n */', '/**\\\\n * Created by 00-00-00 on 08/05/18.\\\\n */', '/**\\\\n * Created by 00-00-00 on 10/05/18.\\\\n */', '/**\\\\n * Example local unit test, which will execute on the development machine (host).\\\\n *\\\\n * See [testing documentation](http://d.android.com/tools/testing).\\\\n */', '/**\\\\n * Instrumented test, which will execute on an Android device.\\\\n *\\\\n * See [testing documentation](http://d.android.com/tools/testing).\\\\n */', '/*\\\\n *  Copyright 2017 Google Inc.\\\\n *\\\\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\\\\n *  you may not use this file except in compliance with the License.\\\\n *  You may obtain a copy of the License at\\\\n *\\\\n *      http://www.apache.org/licenses/LICENSE-2.0\\\\n *\\\\n *  Unless required by applicable law or agreed to in writing, software\\\\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\\\\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\n *  See the License for the specific language governing permissions and\\\\n *  limitations under the License.\\\\n */', '// Context of the app under test.', '// Observe the internal MutableLiveData', '//To change body of created functions use File | Settings | File Templates.', '1', '2', '4', '404', ':', '::', '<', '=', '==', '>', '?', '@', 'API_VERSION', 'Adapter', 'AndroidJUnit4', 'AndroidSchedulers', 'AndroidThreeTen', 'ApiStore', 'AppCompatActivity', 'Application', 'Assert', 'AtomicBoolean', 'BaseActivity', 'BaseFragment', 'BindView', 'BuildConfig', 'Builder', 'Bundle', 'ButterKnife', 'Class', 'CloudStore', 'ColumnInfo', 'Context', 'Customer', 'DB write fail', 'DB write success', 'Dao', 'Database', 'Date', 'DateTimeFormatter', 'DateTypeConverter', 'Disposable', 'Entity', 'Error', 'ExampleInstrumentedTest', 'ExampleUnitTest', 'FORMATTER', 'FROM', 'Flowable', 'Fragment', 'Function', 'GET', 'GsonBuilder', 'GsonConverterFactory', 'HOST', 'HTTP error', 'Hello, Kotlin!', 'HttpStatusOperator', 'HttpStatusOperatorTest', 'ISO_OFFSET_DATE_TIME', 'Insert', 'InstrumentationRegistry', 'Int', 'Intent', 'ItemTransactionBinding', 'JsonDeserializationContext', 'JsonDeserializer', 'JsonElement', 'JsonPrimitive', 'JsonSerializationContext', 'JsonSerializer', 'LENGTH_SHORT', 'LayoutInflater', 'LifecycleOwner', 'LinearLayoutManager', 'List', 'LiveData', 'Loading transactions: ', 'LocalDateTime', 'LocalDateTimeConverter', 'LocalStore', 'Log', 'Long', 'Main2Activity', 'MainActivity', 'MainThread', 'MediaType', 'Multiple observers registered but only one will be notified of changes.', 'MutableLiveData', 'Observable', 'ObservableOperator', 'ObservableSource', 'Observer', 'Oh oh, dum dum ', 'On Error was called', 'On Error was calledq', 'On Success was called', 'OnClick', 'OnConflictStrategy', 'PrimaryKey', 'Query', 'R', 'REPLACE', 'RecyclerView', 'Repository', 'RepositoryImp', 'Response', 'ResponseBody', 'ResponseStatusOperator', 'RetailApplication', 'RetailTransaction', 'RetailTransactionDao', 'Retailer', 'Retrofit', 'Room', 'RoomDatabase', 'RunWith', 'Rx observable triggered ', 'RxJava2CallAdapterFactory', 'SELECT * FROM retail_transactions', 'SELECT * FROM retail_transactions WHERE id = :id LIMIT 1', 'Schedulers', 'SerializedName', 'SingleLiveEvent', 'SingleObserver', 'SingleSource', 'Snackbar', 'Something went wrong', 'String', 'Subscriber', 'System', 'T', 'TAG', 'TODO', 'Test', 'TextView', 'Throwable', 'Toast', 'TransactionListActivity', 'TransactionListAdapter', 'TransactionListItemViewModel', 'TransactionListItemViewModelFactory', 'TransactionListViewModel', 'Type', 'TypeConverter', 'TypeConverters', 'Unit', 'VERTICAL', 'View', 'ViewGroup', 'ViewHolder', 'ViewModel', 'ViewModelProviders', 'Yo, Shits done ', '[', '\\\\n', '\\\\n  ', '\\\\n    ', '\\\\n      ', '\\\\n        ', '\\\\n          ', '\\\\n            ', '\\\\n              ', '\\\\n                ', '\\\\n                    ', '\\\\n                      ', '\\\\n\\\\n', '\\\\n\\\\n  ', '\\\\n\\\\n    ', '\\\\n\\\\n          ', '\\\\n\\\\n\\\\n', '\\\\n\\\\n\\\\n    ', ']', 'a_main2_text', 'a_main_button', 'a_main_container', 'a_main_text', 'a_transaction_list_container', 'a_transaction_list_recycler', 'abstract', 'activity_main', 'activity_main2', 'activity_transaction_list', 'adapt', 'adapter', 'addCallAdapterFactory', 'addConverterFactory', 'addition_isCorrect', 'also', 'amount', 'amount_cents', 'amount_currency', 'android', 'annotation', 'annotations', 'app', 'appContext', 'application/json', 'apply', 'arch', 'asString', 'assert', 'assertEquals', 'atomic', 'baseUrl', 'bind', 'body', 'bp', 'build', 'butterknife', 'by', 'call', 'canonicalName', 'class', 'clazz', 'com', 'com.ground0.transaction', 'companion', 'compareAndSet', 'components', 'concurrent', 'const', 'constructor', 'content', 'context', 'converter', 'core', 'create', 'createItemViewModel', 'createdAt', 'created_at', 'currency', 'currentTimeMillis', 'customer', 'customerId', 'customer_id', 'customers', 'd', 'dao', 'databaseBuilder', 'databaseImp', 'databinding', 'db', 'deserialize', 'design', 'disposables', 'e', 'else', 'email', 'entities', 'error', 'errorEvent', 'false', 'findViewById', 'format', 'from', 'fromCallable', 'fun', 'functions', 'get', 'getCustomer', 'getCustomers', 'getItemCount', 'getNon', 'getRetailer', 'getRetailers', 'getTargetContext', 'getTransaction', 'getTransactions', 'google', 'ground0', 'gson', 'hasActiveObservers', 'holder', 'http', 'id', 'if', 'import', 'inflate', 'init', 'initRecyclerView', 'initViewModel', 'inner', 'insert', 'interface', 'internal', 'io', 'isSuccessful', 'it', 'itemView', 'itemViewModel', 'item_transaction', 'jakewharton', 'java', 'javaClass', 'joinToString', 'json', 'junit', 'kotlinx', 'lang', 'lateinit', 'layout', 'layoutManager', 'lazy', 'let', 'lifecycle', 'list', 'listOf', 'livedata', 'loadTransactions', 'mPending', 'main', 'mainThread', 'make', 'makeText', 'map', 'message', 'model', 'name', 'network', 'non_existant', 'not implemented', 'notifyDataSetChanged', 'null', 'object', 'observe', 'observeOn', 'observer', 'of', 'okhttp3', 'onBindViewHolder', 'onButtonClick', 'onChanged', 'onConflict', 'onCreate', 'onCreateViewHolder', 'onError', 'onSubscribe', 'onSuccess', 'open', 'org', 'os', 'override', 'owner', 'package', 'packageName', 'parent', 'parse', 'persistence', 'phone', 'position', 'postCustomers', 'postRetailer', 'postRetailers', 'postTransaction', 'postTransactions', 'println', 'private', 'protected', 'reactivestreams', 'reactivex', 'recyclerView', 'reflect', 'registerTypeAdapter', 'repository', 'response', 'restImp', 'retailTransaction', 'retailTransactions', 'retail_transactions', 'retailer', 'retailerId', 'retailer_id', 'retailers', 'retrofit', 'retrofit2', 'return', 'room', 'runner', 'rxjava2', 'savedInstanceState', 'schedulers', 'serialize', 'set', 'setContentView', 'setSupportActionBar', 'setValue', 'shouldCallOnErrorFor404', 'shouldCallOnSuccessFor200', 'show', 'showTransactions', 'size', 'src', 'startActivity', 'statusOperator', 'subscribe', 'subscribeOn', 'subscribeToError', 'subscribeToMessages', 'subscribeToTransactions', 'success', 'super', 'support', 'switchMapSingle', 'synthetic', 't', 'tableName', 'test', 'text', 'this', 'threeten', 'threetenabp', 'toDate', 'toObservable', 'toString', 'toolbar', 'transaction', 'transaction/{transactionId}', 'transactionDao', 'transactionId', 'transactionListAdapter', 'transactionListItemViewModelFactory', 'transaction_db', 'transactions', 'true', 'typeOfSrc', 'typeOfT', 'updatedAt', 'updated_at', 'useAppContext', 'util', 'v4', 'v7', 'val', 'value', 'var', 'version', 'view', 'viewModel', 'viewType', 'w', 'widget', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizerValue.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "featuresFileName = \"features_token.tsv\"\n",
    "\n",
    "token_types = vectorizer.get_feature_names()\n",
    "res = vectors.toarray()\n",
    "with open(featuresFileName, 'w') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t', lineterminator='\\n')\n",
    "    tsv_writer.writerow([\"ID\", \"Path\"] + token_types)\n",
    "    i = 1\n",
    "    for line in res:\n",
    "        #print([i] + list(line))\n",
    "        tsv_writer.writerow([i] + [files_list[i - 1]] + list(line))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
